\section{Overview}

\subsection{What is Linear Algebra?}
\begin{itemize}
    \item Linear: The structure of scalar multiplication and addition.
    \item Algebra: Study of formal structures in math. Examples: Groups ($S_n$, $\mathrm{GL}(n, \bb F)$), Rings ($F[X]$, $\bb Z$), and Fields ($\bb R, \bb C$).
\end{itemize}

\subsection{What are Ordinary Differential Equations?}

\subsection{What is the relation between them?}

Solutions to ordinary differential equations form a linear vector space which is a subspace of certain function spaces.

\subsection{Grading Policy}
\begin{itemize}
    \item HW: 0\%
    \item Weekly Quiz: 20\%
    \item Participation: 10\%
    \item Midterms (2): 30\%
    \item Final: 40\%
\end{itemize}

\subsection{Office Hours}
Tuesday 12:30--1:30, Thursday 3--4.

\chapter{Linear Algebra}

\introtext{Class responses to ``What is a vector?'': \begin{enumerate}
    \item Something with magnitude and direction.
    \item A column of numbers.
    \item An element of a vector space.
    \item An ordered collection of magnitudes.
\end{enumerate}}

\section{Vectors in the real plane}

Vectors are commonly defined as arrows on the plane, especially in elementary treatments. The following is a semi-formal treatment of this definition.

In order to define vectors, we must first define what a directed segment is.

\begin{definition}[Directed Segment]
    A directed segment is a line segment between two points with a direction, which can be translated around the plane.\footnote{This is a very informal definition. Formally, we may view directed segments as ordered pairs of points in $\bb R^2$ where the translation operation is an $\bb R^2$-action on the set of directed segments.}
\end{definition}

If a segment starts on point $A$ and ends on point $B$, then we call that directed segment $\vv{AB}$. Now, we can define vectors using these directed segments.

\begin{definition}[Vector]
    Vectors are equivalence classes of directed segments modulo translation. What this means is that if one segment can be translated such that it is equal to another segment, then those segments represent the same vector.
\end{definition}

Now, we can define some basic operations and properties on these vectors.

\begin{enumerate}
    \item Vector addition: Given two vectors $u$ and $v$, we define $u + v$ by choosing directed segments $\vv{AB}$ and $\vv{CD}$ that represent vectors $u$ and $v$, respectively, such that $B = C$. Then, we can concatenate these segments into $\vv{AC}$, then taking the equivalence class $[\vv{AC}]$ to be the sum.
    
    \item Scalar multiplication: Given a vector $v$ and a scalar $\lambda$, we can define scalar multiplication by integers by adding the vector multiple times to itself, and negation as being reflection. Multiplication by rational and irrational numbers are harder to define intuitively, however, it can be defined formally on a vector $[\vv{AB}]$ by $\lambda[\vv{AB}] = [\vv{A'B'}]$ where $A' = \lambda A$ and $B' = \lambda B$.
    
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}[auto]
            \coordinate[label=left:{$A$}] (A) at (0, 0);
            \coordinate[label=above:{$B$}] (B) at (2, 1);
            \coordinate[label=right:{$C$}] (C) at (3, -0.5);
            \draw[-Stealth] (A) to node {$\vv{AB}$} (B);
            \draw[-Stealth] (B) to node {$\vv{BC}$} (C);
            \draw[-Stealth] (A) to node [swap] {$\vv{AC}$} (C);
        \end{tikzpicture}
        \caption{Concatenation of Directed Segments}
    \end{figure}

    \item Vector multiplication? In general, there is no good way to multiply vectors. However, in this example, we can identify $\bb R^2$ with $\bb C$ and define multiplication in that fashion, although this method does not generalize. Vector spaces with a notion of multiplication are called \textit{algebras}.
    
    \item Linear combinations: If $u, v$ are vectors and $a, b \in \mathbb R$, then $au + bv$ is a linear combination of $u$ and $v$.
    
    \item Inner product\footnote{Note: Not all vector spaces have an inner product. Also note that in general, the inner product is defined first, then the norm (magnitude) of the vector is defined as $\sqrt{\braket{v}{v}}$}: A way to ``multiply" two vectors to output a scalar, we can define $\bra{v}\ket{u} \in \mathbb R$ as $\abs{u}\abs{v}\cos(\theta)$, where $\abs{u}$ is the length of a directed segment that represents $u$, and $\theta$ being the angle between representatives of $u$ and $v$ whose starting points are equal. This satisfies bilinearity (linear in both inputs), which means $\braket{au}{w} = a\braket{u}{v}$, and $\braket{u + v}{w} = \braket{u}{w} + \braket{v}{w}$, with analogous identities for the second input.
    
    \item Bases: The standard basis $e_1$ and $e_2$ are represented the unit directed segments pointing to the right and up. Any vector can be written as a linear combination of these two basis vectors. This basis is also orthonormal, i.e. their inner product is $0$ and their lengths are $1$. 
    
    In general, a basis in $\mathbb R^2$ is a set of two vectors that satisfies the property where any vector in $\mathbb R^2$ can be written as a linear combination of those two vectors. In addition, one can prove that as long as two vectors aren't pointing in the exact same or opposite directions, they form a basis.
    
    \item Coordinates: In order to give a vector coordinates (numerical representation), we must specify a basis. After we have specified a basis $e_1, e_2$, vector $v$ has coordinates $(a_1, a_2)$ if $v = a_1e_1 + a_2e_2$.
\end{enumerate}

A special fact about orthonormal bases is that the coordinates of a vector with respect to such a basis corresponds with the inner product of the vector with elements of the basis. This makes it straightforward to compute coordinates as long as you have a formula for the inner product.

\begin{theorem}
    For an orthonormal basis $e_1, e_2$, we have $v = \bra{v}\ket{e_1}e_1 + \bra{v}\ket{e_2}e_2$.
\end{theorem}

\begin{proof}
    $v$ can be decomposed into $ae_1 + be_2$, and thus \begin{align*}
        \braket{ae_1 + be_2}{e_1}e_1 + \braket{ae_1 + be_2}{e_2}e_2 &= \braket{ae_1}{e_1}e_1 + \braket{be_2}{e_1}e_1 + \braket{ae_1}{e_2}e_2 + \braket{be_2}{e_2}e_2\\
        &= a\braket{e_1}{e_1}e_1 + b\braket{e_2}{e_1}e_1 + a\braket{e_1}{e_2}e_2 + b\braket{e_2}{e_2}e_2\\
        &= (a \cdot 1)e_1 + (b \cdot 0)e_1 + (a \cdot 0)e_2 + (b \cdot 1)e_2\\
        &= ae_1 + be_2.
    \end{align*}
\end{proof}

\section{Conic Sections and Quadratic Forms}

Conic sections are the intersections of a infinite double-cone by a plane. The ``shape'' generated is dependent on how tilted this plane is.

\begin{itemize}
    \item Flat plane: Circle
    \item Slightly tilted: Ellipse
    \item Tilted until parallel to slope of cone: Parabola
    \item Tilted past slope of cone: Hyperbola
\end{itemize}

Ellipses on the plane can be defined by fixing two focal points (foci) and specifying a total length, then taking the set of all points such that the distances to the two foci sum to the total length specified.

What's more interesting is that the intersection of a plane to the cone that generates an ellipse allows for two spheres that are tangent to both the plane and the cone, one below and one above the plane. The intersection between those spheres and the plane are the foci of the ellipse. Furthermore, the distance between the two circles of tangency along the cone is the total length. 

Now, we define these objects algebraically:

\begin{definition}[Double Cone]
    The double-cone is defined in $\bb R^3$ as the set of points satisfying $x_1^2 + x_2^2 = x_3^2$.
\end{definition}

\begin{definition}[Plane]
    A plane in $\bb R^3$ is the set of points satisfying $ax_1 + bx_2 + cx_3 = d$.
\end{definition}

Now, to consider the intersection, we consider the solution set to the \emph{system of equations} \[\begin{cases}
    x_1^2 + x_2^2 = x_3^2\\
    ax_1 + bx_2 + cx_3 = d.
\end{cases}\]

How do we solve this system? Substitution. Assuming $c \ne 0$, we can substitute $x_3 = -\frac{1}{c}(ax_1 + bx_2 - d)$ into the first equation and get a quadratic equation in two variables. We then get an equation of the form $Ax_1^2 + Bx_1x_2 + Cx_2^3 + Dx_1 + Ex_2 + F = 0$. This leads us to the following definition:

\begin{definition}[Conic curve]
    A conic curve in $\bb R^2$ is the solution set to $Ax_1^2 + Bx_1x_2 + Cx_2^3 + Dx_1 + Ex_2 + F = 0$.
\end{definition}

\begin{example}[Circle]
    The equation $x_1^2 + x_2^2 - 1 = 0$ defines the unit circle and $(x_1/r)^2 + (x_2/r)^2 - 1 = 0 = x_1^2 + x_2^2 - r^2 = 0$ defines a circle of radius $r$.
\end{example}

\begin{example}[Ellipse]
    The equation $(x_1/r_1)^2 + (x_2/r_2)^2 - 1 = 0$ defines a ellipse with radii $r_1, r_2$.  
\end{example}

\begin{example}[Hyperbola]
    The standard hyperbola is defined as $x_1^2 - x_2^2 - 1 = 0$. Other hyperbolas include $x_1x_2 = 1$, or $x_2^2 - x_1^2 - 1 = 0$
\end{example}

\begin{example}[Parabola]
    $x_1^2 = x_2$.
\end{example}

\begin{theorem}[Classification of conic sections]
    From three basic examples (the unit circle, standard hyperbola, and $x_2 = x_1^2$), we can obtain ALL conic sections by affine linear transformations.
\end{theorem}

\subsubsection*{A short primer on matrix multiplication}

If $A, B$ are matrices of dimensions $n \times \ell$ and $\ell \times m$, then $AB$ is a $n \times m$ matrix, with $(AB)_{ij} = \sum_{k = 1}^\ell A_{ik}B_{kj}$, where $M_{ij}$ is the element in the $i$th row and $j$th column of $M$.

First, we must define what an affine linear transformation is, which requires defining what a regular linear transformation is.

\begin{definition}[Linear transformation]
    $T \colon \bb R^2 \to \bb R^2$ is a linear transformation if it can be expressed as a matrix where \[T\mqty(x_1 \\ x_2) = \mqty(a & b \\ c & d)\mqty(x_1 \\ x_2).\]
\end{definition}

That's kind of a non-intuitive definition. Here's another, more abstract and conceptual definition. This improved definition can also be generalized to any vector space in the domain, and any (not necessarily the same) vector space in the codomain. 

\begin{definition}[Linear Transformation]
    $T \colon \bb R^2 \to \bb R^2$ is a linear transformation if \begin{enumerate}
        \item $T(cv) = cT(v)$.
        \item $T(v + w) = T(v) + T(w)$.
    \end{enumerate}
    In other words, $T$ preserves the linear structure of the space.
\end{definition}

\begin{proposition}
    These definitions are equivalent.
\end{proposition}

\begin{proof}
    It is easy to verify that matrices produce linear transformations through simple computations. To prove that all linear transformations correspond with a matrix, we consider the action on basis vectors. If we fix a basis $v_1, v_2$, since $\smqty(x_1\\ x_2) = x_1v_1 + x_2v_2$, we have \[T\mqty(x_1\\ x_2) = T(x_1v_1 + x_2v_2) = T(x_1v_1) + T(x_2v_2) = x_1T(v_1) + x_2T(v_2).\] In this case, if $T(v_1) = \smqty(a_{11} \\ a_{21})$ and $T(v_2) = \smqty(a_{12} \\ a_{22})$, we then have \[T\mqty(x_1 \\ x_2) = T\mqty(a_{11} & a_{12} \\ a_{21} & a_{22})\mqty(x_1 \\ x_2).\]
\end{proof}

Now, we define affine transformations. But what does ``affine'' mean? Affine spaces like vector spaces with no origin, so you can't add or multiply, but only subtract.

You can make an affine space from the vector space by ``forgetting the origin'', and a vector space from an office space by setting an origin. Subtraction, $- \colon \bb A^2 \times \bb A^2 \to \bb R^2$ gives a vector.

\begin{definition}[Affine Linear Transformation]
    An affine linear transformation is a transformation that preserves the affine structure. 
\end{definition}

Affine linear transformations are precisely linear transformations composed of a translation by a vector.

\begin{theorem}
    Any quadratic form $Q$ can be written in one of the following forms with a correct choice of coordinates
    \begin{enumerate}
        \item $x_1^2 + x_2^2$
        \item $x_1^2 - x_2^2$
        \item $x_1^2$ 
        \item $0$
    \end{enumerate} 
\end{theorem}

\begin{proof}
    If $Q(x_1, x_2) = ax_1^2 + bx_1 + cx_2^2$, then assume $a \ne 0$. Then, we have \[Q(x_1, x_2) = a\qty(x_1 + \frac{b}{2a}x_2)^2 + \qty(c- \frac{b^2}{4a})x_2^2.\]

    From this form, it is easy to convert into one of the preceding forms.
\end{proof}

As a corollary, you can prove the classification of conic sections theorem.