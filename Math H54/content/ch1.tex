\section{Overview}

\subsection{What is Linear Algebra?}
\begin{itemize}
    \item Linear: The structure of scalar multiplication and addition.
    \item Algebra: Study of formal structures in math. Examples: Groups ($S_n$, $\mathrm{GL}(n, \bb F)$), Rings ($F[X]$, $\bb Z$), and Fields ($\bb R, \bb C$).
\end{itemize}

\subsection{What are Ordinary Differential Equations?}

\subsection{What is the relation between them?}

Solutions to ordinary differential equations form a linear vector space which is a subspace of certain function spaces.

\subsection{Grading Policy}
\begin{itemize}
    \item HW: 0\%
    \item Weekly Quiz: 20\%
    \item Participation: 10\%
    \item Midterms (2): 30\%
    \item Final: 40\%
\end{itemize}

\subsection{Office Hours}
Tuesday 12:30--1:30, Thursday 3--4.

\chapter{Linear Algebra}

\introtext{Class responses to ``What is a vector?'': \begin{enumerate}
    \item Something with magnitude and direction.
    \item A column of numbers.
    \item An element of a vector space.
    \item An ordered collection of magnitudes.
\end{enumerate}}

\section{Vectors in the real plane}

Vectors are commonly defined as arrows on the plane, especially in elementary treatments. The following is a semi-formal treatment of this definition.

In order to define vectors, we must first define what a directed segment is.

\begin{definition}[Directed Segment]
    A directed segment is a line segment between two points with a direction, which can be translated around the plane.\footnote{This is a very informal definition. Formally, we may view directed segments as ordered pairs of points in $\bb R^2$ where the translation operation is an $\bb R^2$-action on the set of directed segments.}
\end{definition}

If a segment starts on point $A$ and ends on point $B$, then we call that directed segment $\vv{AB}$. Now, we can define vectors using these directed segments.

\begin{definition}[Vector]
    Vectors are equivalence classes of directed segments modulo translation. What this means is that if one segment can be translated such that it is equal to another segment, then those segments represent the same vector.
\end{definition}

Now, we can define some basic operations and properties on these vectors.

\begin{enumerate}
    \item Vector addition: Given two vectors $u$ and $v$, we define $u + v$ by choosing directed segments $\vv{AB}$ and $\vv{CD}$ that represent vectors $u$ and $v$, respectively, such that $B = C$. Then, we can concatenate these segments into $\vv{AC}$, then taking the equivalence class $[\vv{AC}]$ to be the sum.
    
    \item Scalar multiplication: Given a vector $v$ and a scalar $\lambda$, we can define scalar multiplication by integers by adding the vector multiple times to itself, and negation as being reflection. Multiplication by rational and irrational numbers are harder to define intuitively, however, it can be defined formally on a vector $[\vv{AB}]$ by $\lambda[\vv{AB}] = [\vv{A'B'}]$ where $A' = \lambda A$ and $B' = \lambda B$.
    
    \begin{figure}[h!]
        \centering
        \begin{tikzpicture}[auto]
            \coordinate[label=left:{$A$}] (A) at (0, 0);
            \coordinate[label=above:{$B$}] (B) at (2, 1);
            \coordinate[label=right:{$C$}] (C) at (3, -0.5);
            \draw[-Stealth] (A) to node {$\vv{AB}$} (B);
            \draw[-Stealth] (B) to node {$\vv{BC}$} (C);
            \draw[-Stealth] (A) to node [swap] {$\vv{AC}$} (C);
        \end{tikzpicture}
        \caption{Concatenation of Directed Segments}
    \end{figure}

    \item Vector multiplication? In general, there is no good way to multiply vectors. However, in this example, we can identify $\bb R^2$ with $\bb C$ and define multiplication in that fashion, although this method does not generalize. Vector spaces with a notion of multiplication are called \textit{algebras}.
    
    \item Linear combinations: If $u, v$ are vectors and $a, b \in \mathbb R$, then $au + bv$ is a linear combination of $u$ and $v$.
    
    \item Inner product\footnote{Note: Not all vector spaces have an inner product. Also note that in general, the inner product is defined first, then the norm (magnitude) of the vector is defined as $\sqrt{\braket{v}{v}}$}: A way to ``multiply" two vectors to output a scalar, we can define $\bra{v}\ket{u} \in \mathbb R$ as $\abs{u}\abs{v}\cos(\theta)$, where $\abs{u}$ is the length of a directed segment that represents $u$, and $\theta$ being the angle between representatives of $u$ and $v$ whose starting points are equal. This satisfies bilinearity (linear in both inputs), which means $\braket{au}{w} = a\braket{u}{v}$, and $\braket{u + v}{w} = \braket{u}{w} + \braket{v}{w}$, with analogous identities for the second input.
    
    \item Bases: The standard basis $e_1$ and $e_2$ are represented the unit directed segments pointing to the right and up. Any vector can be written as a linear combination of these two basis vectors. This basis is also orthonormal, i.e. their inner product is $0$ and their lengths are $1$. 
    
    In general, a basis in $\mathbb R^2$ is a set of two vectors that satisfies the property where any vector in $\mathbb R^2$ can be written as a linear combination of those two vectors. In addition, one can prove that as long as two vectors aren't pointing in the exact same or opposite directions, they form a basis.
    
    \item Coordinates: In order to give a vector coordinates (numerical representation), we must specify a basis. After we have specified a basis $e_1, e_2$, vector $v$ has coordinates $(a_1, a_2)$ if $v = a_1e_1 + a_2e_2$.
\end{enumerate}

A special fact about orthonormal bases is that the coordinates of a vector with respect to such a basis corresponds with the inner product of the vector with elements of the basis. This makes it straightforward to compute coordinates as long as you have a formula for the inner product.

\begin{theorem}
    For an orthonormal basis $e_1, e_2$, we have $v = \bra{v}\ket{e_1}e_1 + \bra{v}\ket{e_2}e_2$.
\end{theorem}

\begin{proof}
    $v$ can be decomposed into $ae_1 + be_2$, and thus \begin{align*}
        \braket{ae_1 + be_2}{e_1}e_1 + \braket{ae_1 + be_2}{e_2}e_2 &= \braket{ae_1}{e_1}e_1 + \braket{be_2}{e_1}e_1 + \braket{ae_1}{e_2}e_2 + \braket{be_2}{e_2}e_2\\
        &= a\braket{e_1}{e_1}e_1 + b\braket{e_2}{e_1}e_1 + a\braket{e_1}{e_2}e_2 + b\braket{e_2}{e_2}e_2\\
        &= (a \cdot 1)e_1 + (b \cdot 0)e_1 + (a \cdot 0)e_2 + (b \cdot 1)e_2\\
        &= ae_1 + be_2.
    \end{align*}
\end{proof}

\section{Conic Sections and Quadratic Forms}

Conic sections are the intersections of a infinite double-cone by a plane. The ``shape'' generated is dependent on how tilted this plane is.

\begin{itemize}
    \item Flat plane: Circle
    \item Slightly tilted: Ellipse
    \item Tilted until parallel to slope of cone: Parabola
    \item Tilted past slope of cone: Hyperbola
\end{itemize}

Ellipses on the plane can be defined by fixing two focal points (foci) and specifying a total length, then taking the set of all points such that the distances to the two foci sum to the total length specified.

What's more interesting is that the intersection of a plane to the cone that generates an ellipse allows for two spheres that are tangent to both the plane and the cone, one below and one above the plane. The intersection between those spheres and the plane are the foci of the ellipse. Furthermore, the distance between the two circles of tangency along the cone is the total length. 

Now, we define these objects algebraically:

\begin{definition}[Double Cone]
    The double-cone is defined in $\bb R^3$ as the set of points satisfying $x_1^2 + x_2^2 = x_3^2$.
\end{definition}

\begin{definition}[Plane]
    A plane in $\bb R^3$ is the set of points satisfying $ax_1 + bx_2 + cx_3 = d$.
\end{definition}

Now, to consider the intersection, we consider the solution set to the \emph{system of equations} \[\begin{cases}
    x_1^2 + x_2^2 = x_3^2\\
    ax_1 + bx_2 + cx_3 = d.
\end{cases}\]

How do we solve this system? Substitution. Assuming $c \ne 0$, we can substitute $x_3 = -\frac{1}{c}(ax_1 + bx_2 - d)$ into the first equation and get a quadratic equation in two variables. We then get an equation of the form $Ax_1^2 + Bx_1x_2 + Cx_2^3 + Dx_1 + Ex_2 + F = 0$. This leads us to the following definition:

\begin{definition}[Conic curve]
    A conic curve in $\bb R^2$ is the solution set to $Ax_1^2 + Bx_1x_2 + Cx_2^3 + Dx_1 + Ex_2 + F = 0$.
\end{definition}

\begin{example}[Circle]
    The equation $x_1^2 + x_2^2 - 1 = 0$ defines the unit circle and $(x_1/r)^2 + (x_2/r)^2 - 1 = 0 = x_1^2 + x_2^2 - r^2 = 0$ defines a circle of radius $r$.
\end{example}

\begin{example}[Ellipse]
    The equation $(x_1/r_1)^2 + (x_2/r_2)^2 - 1 = 0$ defines a ellipse with radii $r_1, r_2$.  
\end{example}

\begin{example}[Hyperbola]
    The standard hyperbola is defined as $x_1^2 - x_2^2 - 1 = 0$. Other hyperbolas include $x_1x_2 = 1$, or $x_2^2 - x_1^2 - 1 = 0$
\end{example}

\begin{example}[Parabola]
    $x_1^2 = x_2$.
\end{example}

\begin{theorem}[Classification of conic sections]
    From three basic examples (the unit circle, standard hyperbola, and $x_2 = x_1^2$), we can obtain ALL conic sections by affine linear transformations.
\end{theorem}

\subsubsection*{A short primer on matrix multiplication}

If $A, B$ are matrices of dimensions $n \times \ell$ and $\ell \times m$, then $AB$ is a $n \times m$ matrix, with $(AB)_{ij} = \sum_{k = 1}^\ell A_{ik}B_{kj}$, where $M_{ij}$ is the element in the $i$th row and $j$th column of $M$.

First, we must define what an affine linear transformation is, which requires defining what a regular linear transformation is.

\begin{definition}[Linear transformation]
    $T \colon \bb R^2 \to \bb R^2$ is a linear transformation if it can be expressed as a matrix where \[T\mqty(x_1 \\ x_2) = \mqty(a & b \\ c & d)\mqty(x_1 \\ x_2).\]
\end{definition}

That's kind of a non-intuitive definition. Here's another, more abstract and conceptual definition. This improved definition can also be generalized to any vector space in the domain, and any (not necessarily the same) vector space in the codomain. 

\begin{definition}[Linear Transformation]
    $T \colon \bb R^2 \to \bb R^2$ is a linear transformation if \begin{enumerate}
        \item $T(cv) = cT(v)$.
        \item $T(v + w) = T(v) + T(w)$.
    \end{enumerate}
    In other words, $T$ preserves the linear structure of the space.
\end{definition}

\begin{proposition}
    These definitions are equivalent.
\end{proposition}

\begin{proof}
    It is easy to verify that matrices produce linear transformations through simple computations. To prove that all linear transformations correspond with a matrix, we consider the action on basis vectors. If we fix a basis $v_1, v_2$, since $\smqty(x_1\\ x_2) = x_1v_1 + x_2v_2$, we have \[T\mqty(x_1\\ x_2) = T(x_1v_1 + x_2v_2) = T(x_1v_1) + T(x_2v_2) = x_1T(v_1) + x_2T(v_2).\] In this case, if $T(v_1) = \smqty(a_{11} \\ a_{21})$ and $T(v_2) = \smqty(a_{12} \\ a_{22})$, we then have \[T\mqty(x_1 \\ x_2) = T\mqty(a_{11} & a_{12} \\ a_{21} & a_{22})\mqty(x_1 \\ x_2).\]
\end{proof}

Now, we define affine transformations. But what does ``affine'' mean? Affine spaces like vector spaces with no origin, so you can't add or multiply, but only subtract.

You can make an affine space from the vector space by ``forgetting the origin'', and a vector space from an office space by setting an origin. Subtraction, $- \colon \bb A^2 \times \bb A^2 \to \bb R^2$ gives a vector.

\begin{definition}[Affine Linear Transformation]
    An affine linear transformation is a transformation that preserves the affine structure. 
\end{definition}

Affine linear transformations are precisely linear transformations composed of a translation by a vector.

\begin{theorem}
    Any quadratic form $Q$ can be written in one of the following forms with a correct choice of coordinates
    \begin{enumerate}
        \item $x_1^2 + x_2^2$
        \item $x_1^2 - x_2^2$
        \item $x_1^2$ 
        \item $0$
    \end{enumerate} 
\end{theorem}

\begin{proof}
    If $Q(x_1, x_2) = ax_1^2 + bx_1 + cx_2^2$, then assume $a \ne 0$. Then, we have \[Q(x_1, x_2) = a\qty(x_1 + \frac{b}{2a}x_2)^2 + \qty(c- \frac{b^2}{4a})x_2^2.\]

    From this form, it is easy to convert into one of the preceding forms.
\end{proof}

As a corollary, you can prove the classification of conic sections theorem.

\section{More on Linear Transformations in 2D}

As a reminder, to find a matrix for a transformation $T$ with respect to a basis $e_1, e_2$, we have that $\mqty(Te_1 & Te_2) = \mqty(e_1 & e_2)M$, where $M$ is the matrix representing $T$ in this choice of basis. We then compute \[T(ae_1 + be_2) = T\mqty(e_1 & e_2)\mqty(a \\ b) = \mqty(e_1 & e_2)M\mqty(a \\ b)\]

Examples of linear transformations include 
\begin{enumerate}
    \item Stretching: For example $T(e_1) = 2e_1$, $T(e_2) = \frac{1}{3}e_2$.
    \item Projection: $T(e_1) = 0$, $T(e_2) = 1$.
\end{enumerate}

\subsection{Operations on Linear Transformations}

\begin{definition}[Linear combination of transformations]
    If $T_1$ and $T_2$ are linear maps $\bb R^2 \to \bb R^2$, then $c_1T_1 + c_2T_2$ is defined as the transformation such that \[(c_1T_1 + c_2T_2)v = c_1T_1(v) + c_2T_2(v).\]
\end{definition}

We can see that the matrix representation of $c_1T_1 + c_2T_2$ behaves exactly how we'd expect it to--that is, if $T_1, T_2$ are represented by $M_1$ and $M_2$, then $c_1T_1 + c_2T_2$ is represented by $c_1M_1 + c_2M_2$.

\begin{definition}[Composition]
    If $T_1, T_2$ are transformations then their composition $T_2 \circ T_1$ is the transformation making this diagram commutative \[ 
        \begin{tikzcd}
            \bb R^2 \arrow[r]{T_1} \arrow[rr, bend right = 30]& \bb R^2 \arrow[r] & \bb R^2
        \end{tikzcd}
    \]
    In other words, it is the transformation defined as $(T_2 \circ T_1)(v) = T_2(T_1(v))$.
\end{definition}

The matrix representing the composition is the matrix product.

\begin{definition}[Inverse]
    A transformation is invertible if there exists an inverse. Concretely, if $T$ is a transformation, it is invertible if there exists a transformation $S$ such that $ST = TS = \mathrm{id}$. This transformation $S$ is denoted $T^{-1}$.
\end{definition}

How do we determine whether a transformation is invertible? And how do we compute the matrix of its inverse?

\begin{definition}[Determinant]
    The determinant of a matrix $\smqty(a & b \\ c & d)$, is defined as $\det\smqty(a & b \\ c & d) = ad - bc$.
\end{definition}

\begin{proposition}
    A transformation $T$ is invertible iff $\det([T]) \ne 0$.
\end{proposition}

To compute the inverse of a matrix, we have \[\mqty(a & b \\ c & d)^{-1} = \frac{1}{ad - bc}\mqty(d & -b \\ -c & a)\]

\subsection{Linear groups}

Invertible transformations/matrices, along with the composition operation, satisfy the properties of a group. Composition is associative, and there exists an identity transformation/matrix represented as $\smqty(1 & 0 \\ 0 & 1)$. This group is denoted as $\mathrm{GL}(\bb R^2)$ if we consider transformations and $\mathrm{GL}(2, \bb R)$ for matrices.

An important subgroup of these groups is the orthogonal group $\mathrm O(2)$. This consists of orthogonal transformations.

\begin{definition}[Orthogonal transformation]
    A transformation $O$ is orthogonal if $\braket{v}{w} = \braket{Ov}{Ow}$ for all vectors $v, w$. In other words, it preserves the inner product.
\end{definition}

\begin{proposition}
    A transformation is orthogonal iff $O^\top O = I$.
\end{proposition}

\section{Orthogonal Transformations}

The set of orthogonal transformations on $\bb R^2$ is denoted $\mathrm O(2)$.

We first prove the following proposition:

\begin{proposition}
    If a transformation preserves norms in an inner product space, then it preserves the inner product.
\end{proposition}

\begin{proof}
    $2\braket{u}{v} = \norm{u + v}^2 - \norm{u}^2 - \norm{v}^2$, so the left hand side is preserved if the right hand side is preserved.
\end{proof}

A few more properties about orthogonal transformations are listed below:
\begin{proposition}
    The matrix of an orthogonal transformation $M$ with respect to an orthonormal basis satisfies the property $M^\top M = I$.
\end{proposition}

\begin{proof}
    We have that $\braket{Mv}{Mw} = \braket{v}{M^\top M w} = \braket{v}{w}$. Now, by using projections onto the orthonormal basis vectors, we can prove that $M^\top M = I$.
\end{proof}

A special subset of the orthogonal group is the special orthogonal group $\mathrm{SO}(2)$, which induces the additional constraint that $\det(O) = 1$, and consists of plane rotations about the origin.

\subsection{Complex Numbers}

The imaginary unit $i$ is defined as a solution of $x^2 + 1 = 0$, and the complex numbers are defined as numbers of the form $a + bi$.

Multiplication is then easily defined as $(a + bi)(c + di) = ac - bd + (ad + bc)i$.

Complex numbers have a length and an angle, and can be expressed in terms of the length and angle using the formula $e^{i\theta} = \cos(\theta) + i\sin(\theta)$. 

We can see that the group $\mathrm{SO}(2)$ is isomorphic to the unit complex numbers under multiplication.

\section{The Main Theorems of Linear Algebra}

In linear algebra, we have four main theorems:

\begin{itemize}
    \item Rank Theorem
    \item Inertia Theorem
    \item Orthogonal Diagonalization Theorem
    \item Jordan Normal Form
\end{itemize}

\begin{definition}[Classification Theorem]
    Classification theorems in linear algebra seek to classify and find ``nice" representatives of the orbits of group actions.
\end{definition}

\begin{theorem}[Rank theorem]
    Any linear equation of the form $y = Ax$ can be simplified by a change of variables into the form $\tilde y = M\tilde x$, where $M$ is a  block matrix of the form \[\mqty(I & 0 \\ 0 & 0).\]
\end{theorem}

\begin{theorem}[Inertia theorem]
    Any quadratic form can be rewritten after a change of variables into the form $x_1^2 + \cdots + x_p^2 - x_{p + 1}^2 + \cdots + x_q^2$.
\end{theorem}

\begin{theorem}
    
\end{theorem}

\begin{theorem}[Jordan Normal Form]
    Any matrix can be rewritten as a change of variables into a diagonal block matrix consisting of Jordan blocks, which are of the form \[\mqty(\lambda & 1 &  & 0\\
    & \ddots & \ddots & \\
    & & \lambda & 1 \\ 0 & & & \lambda )\]
\end{theorem}

\section{Matrices in Higher Dimensions}

If $A$ is a matrix, we use $A_{ij}$ to refer to the $i$th row and $j$th column of the matrix.

Then, we are able to define operations on matrices:

\begin{definition}[Transpose]
    The transpose of a $m \times n$ matrix $A$, denoted by $A^\top$, is the $n \times m$ matrix defined by $(A^\top)_{ij} = A_{ji}$.
\end{definition}

\begin{definition}[Multiplication]
    If $A$ is a $n \times m$ matrix and $B$ is a $m \times \ell$ matrix, then $AB$ is a $n \times \ell$ matrix with $(AB)_{ij} = A_{i1}B_{1j} + \cdots + A_{im}B_{mj}$. Using summation, we have that \[(AB)_{ij} = A_{ik}B_{kj} = \sum_{k = 1}^m A_{ik}B_{kj}.\]
\end{definition}

Why do we define multiplication of matrices like this? Multiplication of matrices correspond to compositions of linear transformations. An example of an application is a neural network, where ``neurons'' activity is based off matrices, and these are composed to get a final result.

\subsection{Determinant}

\begin{definition}[Determinant]
    If $A$ is an $n \times n$ matrix, then the determinant of $A$, is a number defined as follows: \[\det(A) = \sum_{\sigma \in S_n} \mathrm{sgn}(\sigma)\prod_{i = 1}^n A_{i\sigma(i)}.\]
\end{definition}

$S_n$ denotes the symmetric group on $n$ elements, or the permutations of $n$ elements, which has size $n!$. $\mathrm{sgn}(\sigma)$ is the ``sign'' of the permutation, which is $1$ if the permutation swaps the order of an even number of elements and $-1$ otherwise.